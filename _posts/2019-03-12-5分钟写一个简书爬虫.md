![image.png](https://upload-images.jianshu.io/upload_images/16432686-5716820c15444fc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
#### 工具准备
+ python2.x 3.x 都行
+ bs4（BeautifulSoup） 解析html
+ requests  网络请求模块  

#### 大致流程
###### 1、获取网页
用 request.get("https://www.jianshu.com/u/b8ae5468c7b6") 获取页面的源代码到本地
```
baseUrl = "https://www.jianshu.com/u/b8ae5468c7b6"
#请求网页
request = requests.get(baseUrl)
#打印获取到的内容
print(request.text)
```
直接获取得到的结果是 ***403 Forbidden***
![失败结果.png](https://upload-images.jianshu.io/upload_images/16432686-b761eb7ed15c21a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

解决办法：第一想到的肯定是加 headers 啊。
```
baseUrl = "https://www.jianshu.com/u/b8ae5468c7b6"
#设置头信息
headers = {
"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"
}
#请求网页
request = requests.get(baseUrl, headers=headers)
#打印获取到的内容
print(request.text)
```
为什么加 headers 就能成功访问？当然是为了欺骗服务器啊，让对端服务器以为你就是正常的浏览器在访问，从而不对你进行拦截。

###### 2、分析网页
需要拿的数据：  
+ 文章总数(articleNums)
+ 文章标题(title)
+ 文章链接(href)  

尝试第一次，使用这个地址 https://www.jianshu.com/u/b8ae5468c7b6 只能拿到九条数据，看下页面上也没有分页的bar，那么肯定是鼠标滚动控制刷新页面。  
用火狐浏览器的 firebug 监听滚鼠标时的网络情况，可以发现如下链接：  
![firebug.png](https://upload-images.jianshu.io/upload_images/16432686-2142e989a8b9c75b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

即 https://www.jianshu.com/u/b8ae5468c7b6?order_by=shared_at&page=1  
手动修改page的值页面发生变化，那么现在只需要找到文章一共多少页就可以成功翻页了。  

一页9条，一共22条，那么肯定有三页。  
分析完成，接下来用 BeautifulSoup 来抽取想要的值 。
###### 3、用 BeautifulSoup 来抽取想要的值    
获取文章数：
```
baseUrl = "https://www.jianshu.com/u/b8ae5468c7b6"
#设置头信息
headers = {
"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"
}
#请求网页
request = requests.get(baseUrl, headers=headers)
#加载网页到bs中
html = BeautifulSoup(req.text, 'html.parser')
#获取文章总数
articleNums = html.find_all("div", {"class": "meta-block"})[2].find("p").text
articleNums = int(articleNums)
print(articleNums)
```
获取标题、文章链接：  
```
baseUrl = "https://www.jianshu.com/u/b8ae5468c7b6"
#设置头信息
headers = {
"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"
}
#请求网页
request = requests.get(baseUrl, headers=headers)
#加载网页到bs中
html = BeautifulSoup(req.text, 'html.parser')
#获取标题、文章链接：
ul = html.find("ul", {"class": "note-list"})
lis = ul.find_all("li")
for li in lis:
    titleBlock = li.find("a", {"class": "title"})
    title = titleBlock.text
    href = baseUrl + titleBlock['href']
    print(title, href)
```
运行结果：
  
![运行结果.png](https://upload-images.jianshu.io/upload_images/16432686-a3c8ec6293802b70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 完整代码
```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
@Time    : 2019/3/13 14:49
@Author  : Negen
@Site    :
@File    : jianshu.py
@Software: PyCharm
'''
import requests
from bs4 import BeautifulSoup
#url = 'https://www.jianshu.com/u/b8ae5468c7b6'
#url = 'https://www.jianshu.com/u/b8ae5468c7b6?order_by=shared_at&page=1'
headers = {
"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"
}
baseUrl = "https://www.jianshu.com"
userArticleListBaseUrl = 'https://www.jianshu.com/u/b8ae5468c7b6'
req = requests.get(userArticleListBaseUrl, headers=headers)
html = BeautifulSoup(req.text, 'html.parser')
articleNums = html.find_all("div", {"class": "meta-block"})[2].find("p").text
articleNums = int(articleNums)
pages = articleNums//9
if pages * 9 < articleNums:
    pages += 1
for pageNum in range(1, pages + 1):
    articleListUrl = userArticleListBaseUrl + '?order_by=shared_at&page=' + str(pageNum)
    req = requests.get(articleListUrl, headers=headers)
    html = BeautifulSoup(req.text, 'html.parser')
    ul = html.find("ul", {"class":"note-list"})
    lis = ul.find_all("li")
    for li in lis:
        titleBlock = li.find("a", {"class": "title"})
        title = titleBlock.text
        href = baseUrl + titleBlock['href']
        print(title, href)
```
